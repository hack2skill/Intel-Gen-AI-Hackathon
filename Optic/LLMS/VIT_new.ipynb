{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f9e8137-b37a-4863-9bf7-040099996f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u6f6f813397ee377507e94d05d7e4098/.local/lib/python3.9/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoFeatureExtractor\n",
    "from intel-extension-for-transformers.transformers.pipeline import pipeline\n",
    "from intel_extension_for_transformers.transformers.trainer import Seq2SeqTrainer\n",
    "from transformers import Seq2SeqTrainingArguments,default_data_collator\n",
    "encoder_checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "decoder_checkpoint = \"gpt2\"\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(encoder_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(decoder_checkpoint)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6183075-2cb0-4709-942a-aa76a0a5ce80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 23:04:43,682 - numexpr.utils - INFO - Note: detected 224 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "2023-12-04 23:04:43,683 - numexpr.utils - INFO - Note: NumExpr detected 224 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2023-12-04 23:04:43,683 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('Fashion Dataset v2 - Fashion Dataset v2.csv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0107da25-39a9-41e1-ac9a-d65fed4f1484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://assets.myntassets.com/assets/images/170...</td>\n",
       "      <td>Black printed Kurta with Palazzos with dupatta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://assets.myntassets.com/assets/images/165...</td>\n",
       "      <td>Orange solid Kurta with Palazzos with dupatta&lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://assets.myntassets.com/assets/images/163...</td>\n",
       "      <td>Navy blue embroidered Kurta with Trousers with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://assets.myntassets.com/assets/images/147...</td>\n",
       "      <td>Red printed kurta with trouser and dupatta&lt;br&gt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://assets.myntassets.com/assets/images/110...</td>\n",
       "      <td>Black and green printed straight kurta, has a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14209</th>\n",
       "      <td>http://assets.myntassets.com/assets/images/154...</td>\n",
       "      <td>Blue solid front-open sweatshirt has a mock co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14210</th>\n",
       "      <td>http://assets.myntassets.com/assets/images/164...</td>\n",
       "      <td>Green printed sweatshirt has a hooded,  2 pock...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14211</th>\n",
       "      <td>http://assets.myntassets.com/assets/images/163...</td>\n",
       "      <td>Pink solid sweatshirt has a mock collar,  2 ka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14212</th>\n",
       "      <td>http://assets.myntassets.com/assets/images/163...</td>\n",
       "      <td>Blue solid sweatshirt has a round neck, long s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14213</th>\n",
       "      <td>http://assets.myntassets.com/assets/images/163...</td>\n",
       "      <td>Grey and green colourblocked sweatshirt has a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14214 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     img  \\\n",
       "0      http://assets.myntassets.com/assets/images/170...   \n",
       "1      http://assets.myntassets.com/assets/images/165...   \n",
       "2      http://assets.myntassets.com/assets/images/163...   \n",
       "3      http://assets.myntassets.com/assets/images/147...   \n",
       "4      http://assets.myntassets.com/assets/images/110...   \n",
       "...                                                  ...   \n",
       "14209  http://assets.myntassets.com/assets/images/154...   \n",
       "14210  http://assets.myntassets.com/assets/images/164...   \n",
       "14211  http://assets.myntassets.com/assets/images/163...   \n",
       "14212  http://assets.myntassets.com/assets/images/163...   \n",
       "14213  http://assets.myntassets.com/assets/images/163...   \n",
       "\n",
       "                                             description  \n",
       "0      Black printed Kurta with Palazzos with dupatta...  \n",
       "1      Orange solid Kurta with Palazzos with dupatta<...  \n",
       "2      Navy blue embroidered Kurta with Trousers with...  \n",
       "3      Red printed kurta with trouser and dupatta<br>...  \n",
       "4      Black and green printed straight kurta, has a ...  \n",
       "...                                                  ...  \n",
       "14209  Blue solid front-open sweatshirt has a mock co...  \n",
       "14210  Green printed sweatshirt has a hooded,  2 pock...  \n",
       "14211  Pink solid sweatshirt has a mock collar,  2 ka...  \n",
       "14212  Blue solid sweatshirt has a round neck, long s...  \n",
       "14213  Grey and green colourblocked sweatshirt has a ...  \n",
       "\n",
       "[14214 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbf605a5-2a0d-4604-8e8a-ee715b52ead5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "max_length = 128\n",
    "sample = df.iloc[0]\n",
    "\n",
    "# sample image\n",
    "image = Image.open(requests.get(sample['img'],stream=True).raw)\n",
    "# sample caption\n",
    "caption = sample['description']\n",
    "\n",
    "# apply feature extractor on the sample image\n",
    "inputs = feature_extractor(images=image, return_tensors='pt')\n",
    "# apply tokenizer\n",
    "outputs = tokenizer(\n",
    "            caption, \n",
    "            max_length=max_length, \n",
    "            truncation=True, \n",
    "            padding='max_length',\n",
    "            return_tensors='pt',\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe949325-bbae-4b37-8da1-e2605729c34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pixel_values': tensor([[[[0.3020, 0.3020, 0.3020,  ..., 0.5137, 0.5216, 0.5216],\n",
      "          [0.3020, 0.3020, 0.3020,  ..., 0.5137, 0.5137, 0.5216],\n",
      "          [0.3020, 0.3020, 0.3020,  ..., 0.5137, 0.5137, 0.5137],\n",
      "          ...,\n",
      "          [0.5137, 0.5137, 0.5137,  ..., 0.4196, 0.4196, 0.4196],\n",
      "          [0.5137, 0.5137, 0.5059,  ..., 0.4275, 0.4275, 0.4275],\n",
      "          [0.5059, 0.5059, 0.5137,  ..., 0.4275, 0.4275, 0.4275]],\n",
      "\n",
      "         [[0.3020, 0.3020, 0.3020,  ..., 0.5059, 0.5137, 0.5137],\n",
      "          [0.3020, 0.3020, 0.3020,  ..., 0.5137, 0.5137, 0.5137],\n",
      "          [0.3020, 0.3020, 0.3020,  ..., 0.5216, 0.5216, 0.5216],\n",
      "          ...,\n",
      "          [0.5137, 0.5137, 0.5137,  ..., 0.4196, 0.4196, 0.4196],\n",
      "          [0.5137, 0.5137, 0.5059,  ..., 0.4275, 0.4275, 0.4275],\n",
      "          [0.5059, 0.5059, 0.5137,  ..., 0.4275, 0.4275, 0.4275]],\n",
      "\n",
      "         [[0.3176, 0.3176, 0.3176,  ..., 0.5451, 0.5529, 0.5529],\n",
      "          [0.3176, 0.3176, 0.3176,  ..., 0.5529, 0.5529, 0.5529],\n",
      "          [0.3176, 0.3176, 0.3176,  ..., 0.5529, 0.5529, 0.5529],\n",
      "          ...,\n",
      "          [0.5294, 0.5294, 0.5294,  ..., 0.4353, 0.4353, 0.4353],\n",
      "          [0.5294, 0.5294, 0.5216,  ..., 0.4431, 0.4431, 0.4431],\n",
      "          [0.5216, 0.5216, 0.5294,  ..., 0.4431, 0.4431, 0.4431]]]])}\n"
     ]
    }
   ],
   "source": [
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f82528d-e36b-4467-81c5-edfe5ce92f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 9915, 10398, 20642,    64,   351,  3175,  8101,   418,   351, 32597,\n",
      "         25014,  1279,  1671,    29,  1279,  1671,    29,  1279,    65,    29,\n",
      "         20642,    64,  1486,    25,   220,  7359,    65,    29,  1279,   377,\n",
      "            29,  1279,  4528,    29, 48021, 32702,    82, 10398,  7359,  4528,\n",
      "            29,  1279,  4528,    29,  1052,   668,  7344,  5485,  7359,  4528,\n",
      "            29,  1279,  4528,    29, 23603,  3918,  7359,  4528,    29,  1279,\n",
      "          4528,    29, 41621, 19908,    11,   220,  1115,    12, 24385,  3218,\n",
      "         27409,  7359,  4528,    29,  1279,  4528,    29,   327,  1604,  4129,\n",
      "           351, 50017, 16869,  7359,  4528,    29,  1279,  4528,    29,   569,\n",
      "          2304,   577, 26842,   261,  4572, 37982,  9664,  7359,  4528,    29,\n",
      "          7359,   377,    29,  1279,  1671,    29,  1279,    65,    29,  3175,\n",
      "          8101,   418,  1486,    25,   220,  7359,    65,    29,  1279,   377,\n",
      "            29,  1279,  4528,    29, 38482,  3175,  8101,   418]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f0845c5-cbac-49f6-8a83-b9b6b3e1bd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class LoadDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.images = df['img'].values\n",
    "        self.captions = df['description'].values\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        inputs = dict()\n",
    "\n",
    "        # load the image and apply feature_extractor\n",
    "        image_path = str(self.images[idx])\n",
    "        image =  Image.open(requests.get(image_path,stream=True).raw)\n",
    "        image = feature_extractor(images=image, return_tensors='pt')\n",
    "\n",
    "        # load the caption and apply tokenizer\n",
    "        caption = self.captions[idx]\n",
    "        labels = tokenizer(\n",
    "            caption, \n",
    "            max_length=max_length, \n",
    "            truncation=True, \n",
    "            padding='max_length',\n",
    "            return_tensors='pt',\n",
    "        )['input_ids'][0]\n",
    "        \n",
    "        # store the inputs(pixel_values) and labels(input_ids) in the dict we created\n",
    "        inputs['pixel_values'] = image['pixel_values'].squeeze()   \n",
    "        inputs['labels'] = labels\n",
    "        return inputs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a03508b7-dbc3-4719-b6da-29b6b1a8866b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d78dc997-111f-49fa-bf4f-936d1dd97936",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = LoadDataset(train_df)\n",
    "test_ds = LoadDataset(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d398bd66-1d68-4f08-83d0-42fccc49746a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.7.crossattention.q_attn.bias', 'h.4.crossattention.c_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.1.crossattention.q_attn.weight', 'h.8.ln_cross_attn.weight', 'h.5.ln_cross_attn.bias', 'h.5.ln_cross_attn.weight', 'h.2.crossattention.q_attn.bias', 'h.11.crossattention.c_proj.bias', 'h.8.crossattention.c_attn.bias', 'h.0.ln_cross_attn.weight', 'h.1.ln_cross_attn.bias', 'h.2.ln_cross_attn.bias', 'h.3.crossattention.c_attn.bias', 'h.1.crossattention.q_attn.bias', 'h.0.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.bias', 'h.11.crossattention.q_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.9.crossattention.q_attn.weight', 'h.4.crossattention.q_attn.weight', 'h.2.ln_cross_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.4.ln_cross_attn.bias', 'h.10.crossattention.c_proj.bias', 'h.1.crossattention.c_attn.weight', 'h.3.ln_cross_attn.weight', 'h.8.ln_cross_attn.bias', 'h.3.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.weight', 'h.10.crossattention.q_attn.bias', 'h.6.ln_cross_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.10.ln_cross_attn.weight', 'h.9.crossattention.q_attn.bias', 'h.5.crossattention.q_attn.bias', 'h.3.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.0.ln_cross_attn.bias', 'h.8.crossattention.q_attn.bias', 'h.0.crossattention.c_proj.bias', 'h.7.crossattention.c_proj.weight', 'h.9.crossattention.c_attn.bias', 'h.9.crossattention.c_proj.weight', 'h.2.crossattention.c_attn.weight', 'h.7.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.8.crossattention.q_attn.weight', 'h.6.ln_cross_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.weight', 'h.11.crossattention.c_attn.weight', 'h.3.crossattention.q_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.6.crossattention.q_attn.weight', 'h.3.crossattention.c_attn.weight', 'h.7.ln_cross_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.0.crossattention.c_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.0.crossattention.c_proj.weight', 'h.6.crossattention.c_proj.weight', 'h.5.crossattention.c_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.6.crossattention.q_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.10.ln_cross_attn.bias', 'h.9.crossattention.c_proj.bias', 'h.2.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.bias', 'h.2.crossattention.c_proj.weight', 'h.11.crossattention.c_proj.weight', 'h.9.crossattention.c_attn.weight', 'h.4.crossattention.q_attn.bias', 'h.4.ln_cross_attn.weight', 'h.7.ln_cross_attn.weight', 'h.3.ln_cross_attn.bias', 'h.9.ln_cross_attn.bias', 'h.7.crossattention.c_proj.bias', 'h.9.ln_cross_attn.weight', 'h.5.crossattention.q_attn.weight', 'h.10.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.weight', 'h.8.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.weight', 'h.0.crossattention.q_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.10.crossattention.c_proj.weight', 'h.11.ln_cross_attn.weight', 'h.1.ln_cross_attn.weight', 'h.0.crossattention.q_attn.bias', 'h.11.ln_cross_attn.bias', 'h.1.crossattention.c_proj.bias', 'h.10.crossattention.c_attn.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import VisionEncoderDecoderModel\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    encoder_checkpoint, \n",
    "    decoder_checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b860f7d-05e2-44c0-93a8-0326f983aa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "DEVICE = torch.device(\"xpu\" if torch.xpu.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dfa53d-4cc7-40f1-9647-a6210f8213dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='117' max='890' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [117/890 2:17:06 < 15:21:33, 0.01 it/s, Epoch 0.65/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "bs = 64\n",
    "model.to(DEVICE)\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"image-caption-generator\", # name of the directory to store training outputs\n",
    "    evaluation_strategy=\"epoch\",          # evaluate after each epoch\n",
    "    per_device_train_batch_size=bs,       # batch size during training\n",
    "    per_device_eval_batch_size=bs,        # batch size during evaluation\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,                    # weight decay parameter for AdamW optimizer\n",
    "    num_train_epochs=5,                   # number of epochs to train\n",
    "    save_strategy='epoch',                # save checkpoints after each epoch\n",
    "    report_to='none',                     # prevent reporting to wandb, mlflow...\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=feature_extractor,\n",
    "    data_collator=default_data_collator,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86852355-e8c3-4f94-88d4-3c262e4a2d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('image-caption-gen')\n",
    "tokenizer.save_pretrained('image-caption-gen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b811b90f-5d2d-4842-987d-0561bfa33fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "df = pd.read_csv(\"FashionV2.csv\")\n",
    "\n",
    "column = df['img'].values\n",
    "time_per_step = []\n",
    "\n",
    "complete_start_time = time.time()\n",
    "text_classifier = pipeline(\n",
    "    task=\"image-to-text\",\n",
    "     model=\"image-caption-gen/\",\n",
    "    framework=\"pt\",\n",
    "    device=torch.device(\"cpu\"),\n",
    ")\n",
    "for data in tqdm(column[:1000]):\n",
    "    a = text_classifier(data)\n",
    "\n",
    "complete_end = time.time()\n",
    "complete_time_taken = complete_end - complete_start_time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
