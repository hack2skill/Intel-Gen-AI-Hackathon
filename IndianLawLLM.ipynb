{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57b3de00-61f6-418a-b837-57a207ee1c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain -q --user "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8c66634-87ee-4799-a29a-e9505b1edc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers -q --user "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0064e947-4ded-4bce-925b-66042f25f063",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pypdf -q --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "367609ba-2daf-458d-9e31-b0bfc3b96f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers -q --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ea8f326-4535-41df-9c04-8b094c43072b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromadb -q --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5102e563-bbd0-4cac-9c58-a4a64ba82e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-cloud -q --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42f71c23-935e-44a0-894a-34af021f5955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting intel_extension_for_pytorch\n",
      "  Downloading intel_extension_for_pytorch-2.1.0-cp39-cp39-manylinux2014_x86_64.whl (51.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from intel_extension_for_pytorch) (5.9.0)\n",
      "Requirement already satisfied: numpy in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from intel_extension_for_pytorch) (1.24.3)\n",
      "Requirement already satisfied: packaging in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from intel_extension_for_pytorch) (23.1)\n",
      "Installing collected packages: intel_extension_for_pytorch\n",
      "\u001b[33m  WARNING: The script ipexrun is installed in '/home/ub9a04e0e61464b0670ffe393523c081/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed intel_extension_for_pytorch-2.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install intel_extension_for_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1261631-421a-4df5-85ce-c6b1b64f642d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8132a8ff-2c53-4ed7-ab16-afc1695c1ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress warnings for a cleaner output\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b1fe6e1-fb17-40ba-9cd4-ccdf4905dc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bd92eb1-f9ed-4518-a536-95654b75f92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import intel_extension_for_pytorch as ipex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e6c20bb-7299-4057-9b7e-2611a12d6105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules and classes\n",
    "from langchain.vectorstores import Chroma\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM \n",
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a389773-65bb-4318-9d17-b69d60459c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_system_config():\n",
    "    system_type = None\n",
    "    is_xpu = False\n",
    "    is_gpu = False\n",
    "    ret_val = {}\n",
    "    # random seed\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"GPU is available\")\n",
    "        system_type = \"GPU\"\n",
    "        is_gpu = True\n",
    "        seed = 88\n",
    "        random.seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    else:\n",
    "        if torch.xpu.is_available():\n",
    "            print(\"GPU is not available\")\n",
    "            print(\"XPU is available\")\n",
    "            system_type = \"XPU\"\n",
    "            is_xpu = True\n",
    "            seed = 88\n",
    "            random.seed(seed)\n",
    "            torch.xpu.manual_seed(seed)\n",
    "            torch.xpu.manual_seed_all(seed)\n",
    "        else:\n",
    "             print(\"XPU is not available\")\n",
    "        \n",
    "\n",
    "    ret_val = {\"is_xpu\":is_xpu, \"is_gpu\": is_gpu, \"system_type\":system_type}\n",
    "    print(f\"System Config: {ret_val}\")\n",
    "    return ret_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "447e72b8-c930-4e88-a823-4ebb1575d06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XPU is not available\n",
      "System Config: {'is_xpu': False, 'is_gpu': False, 'system_type': None}\n",
      "{'is_xpu': False, 'is_gpu': False, 'system_type': None}\n"
     ]
    }
   ],
   "source": [
    "system_config = check_system_config()\n",
    "print(system_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a99ad4b-69a9-4e20-ace7-abe907b82d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='xpu')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"xpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc4a34b-1a17-468d-b999-9934091321f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78f669e5-3be9-4e34-bb89-607018c24f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a directory loader to load PDF documents from a directory# Define the directory where the Chroma database will persist its data\n",
    "persist_directory = \"choma-db\"\n",
    "\n",
    "# Make sure the directory exists, create if it doesn't\n",
    "import os\n",
    "if not os.path.exists(persist_directory):\n",
    "    os.makedirs(persist_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9edd645e-d697-4f58-a804-fa1fac43ec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a directory loader to load PDF documents from a directory\n",
    "loader = DirectoryLoader(\"law_data\", glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
    "documents = loader.load()\n",
    "\n",
    "# Initialize a text splitter to split documents into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=200)\n",
    "\n",
    "# Split the loaded documents into chunks\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75299674-1e81-432a-9e29-658a9d46c96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba6255fb-e6a4-482e-a6bd-94a32e84e975",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-05 18:12:37,853 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: multi-qa-mpnet-base-dot-v1\n",
      "2023-12-05 18:12:38,583 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Creating a Vector DB using Chroma DB and SentenceTransformerEmbeddings\n",
    "# Initialize SentenceTransformerEmbeddings with a pre-trained model\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=\"multi-qa-mpnet-base-dot-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f8ff666-0155-486b-b034-4e81d556186c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-05 18:12:43,870 - chromadb.telemetry.product.posthog - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a985512b7b044dc39459716736c55de9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/474 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a Chroma vector database from the text chunks\n",
    "db = Chroma.from_documents(texts, embeddings, persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98ef8ea4-5837-404f-bfaf-de622eb9d55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save and load the saved vector db (if needed in the future)\n",
    "# Persist the database to disk\n",
    "db.persist()\n",
    "db = Chroma(persist_directory=persist_directory, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9bd72ea5-d502-4049-ab00-c1b69ae53834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06e115543eac4056bbe0bf9dcfebde16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ee76a081e924bd5933dcc7e45a17cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acc49c0a14f8471a827d644e805938f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c50f74c1068435a8537c91bb1470bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d6efed1b824db2861b17bd3d234e80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/860 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acd5fbe52b304b0aa1784fdb2e9f1466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96040b1f37384665bc4ef3daeea50888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Specify the checkpoint for the language model\n",
    "checkpoint = \"MBZUAI/LaMini-Flan-T5-783M\"\n",
    "\n",
    "# Initialize the tokenizer and base model for text generation\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    checkpoint,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "536e4798-e9b1-46c5-a793-59ac6ff5797e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text generation pipeline\n",
    "pipe = pipeline(\n",
    "    'text2text-generation',\n",
    "    model = base_model,\n",
    "    tokenizer = tokenizer,\n",
    "    max_length = 512,\n",
    "    do_sample = True,\n",
    "    temperature = 0.3,\n",
    "    top_p= 0.95\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b13371d3-df88-46df-9f0a-c19e3e154255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RetrievalQA chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71022a3c-9b90-479b-b894-3ea9ef4977f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a local language model pipeline\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "# Create a RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=local_llm,\n",
    "    chain_type='stuff',\n",
    "    retriever=db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2}),\n",
    "    return_source_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b8f6f34-52d9-4336-b74d-993f08923529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your query: what is Indian penal code\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6563b7a03ab1477d8ba26aec9bfe491c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Indian penal code is a comprehensive interpretation clause that defines and explains the leading terms used in the chapter, and the meanings announced are consistently adhered to throughout the subsequent chapters.\n"
     ]
    }
   ],
   "source": [
    "# Prompt the user for a query\n",
    "input_query = str(input(\"Enter your query:\"))\n",
    "\n",
    "# Execute the query using the QA chain\n",
    "llm_response = qa_chain({\"query\": input_query})\n",
    "\n",
    "# Print the response\n",
    "print(llm_response['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "21ba0762-c204-423c-8433-3d68cdb60b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your query: explain ipc 1860\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3cd5df17b8241a18f426b183ebe624e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPC, 1860 is a civil law code in India that provides a framework for criminal proceedings and punishments. It includes provisions for territorial operation of the code, general explanations, penalties, exceptions, general exceptions, and specific offences. The conviction of an appellant under section 302 of the code cannot be set aside merely for the absence of framing of a specific/alternate charge for an offense punishable under that section.\n"
     ]
    }
   ],
   "source": [
    "# Prompt the user for a query\n",
    "input_query = str(input(\"Enter your query:\"))\n",
    "\n",
    "# Execute the query using the QA chain\n",
    "llm_response = qa_chain({\"query\": input_query})\n",
    "\n",
    "# Print the response\n",
    "print(llm_response['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbff479-17d8-4e90-a286-af71d1c74bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
